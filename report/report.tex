\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{bigfoot}
\usepackage{hyperref}

\title{A Short Report of My Solution to the Assignment}

\author{Wei Qiu}

\date{\today}

\begin{document}
\maketitle

\section{Directory structure}
I reorganized the files to make my solution easier to explain.
\begin{itemize}
    \item All the data is located at data.
    \item Test data is in data/test. And test.en is copied to reference0.
    \item Train data is in data/train. 
    \item Dev data is in data/dev.
\end{itemize}

\section{Problem 1 \& 3}
The config file  for the baseline system is located at  baseline/config.
The config file for the baseline system with further tuning on  development set is located at baseline-tune/config.

The output is located at evaluation directory correspondingly.
My first attempt of using mert for tuning(which I used for my previous project) crashed surprisingly. I turned to the moses-support mailing list for help.



\section{Problem 2}
The first idea comes to my mind is to simply append all of the terminology table to the training table. The sript split\_term.py is used to split the terminology into seperate files. Then I manually append them to the corresponding training files.

The trained model using this idea is located at prob2. The corresponding training data is at data/train\_term.

A further research shows that moses has an advanced feature that can let the user specify how to translate certain words by marking up them during preprocessing.\footnote{\url{http://www.statmt.org/moses/?n=Moses.AdvancedFeatures#ntoc11}}
This may be the best approach to deal with external terminology dictionary.




\section{Problem 5: further improvement}
There are at least two ways to improve the translation quality:
\begin{itemize}
    \item As German is an agglutinative language, splitting up the compound words can reduce the sparsity. We can already find a lot of OOV words in the output of baseline system.
    \item As the word order in German has much more freedom than English, a preprocessing step which always normalize German sentence to SVO can reduce the sparsity as well.
\end{itemize}
For the first idea, I found tools such as jwordsplitter\footnote{\url{http://www.danielnaber.de/jwordsplitter/index_en.html}} and compound-splitter\footnote{\url{https://github.com/dweiss/compound-splitter}}. 
For the second idea, I found that the idea has been explored since 2005.


\section{Result}
\begin{table}
    \centering
    \begin{tabular}{l|r|r|r|r}
        Name & BLEU & METEOR & GTM & TER \\
        Baseline & & & & \\
        Baseline+Tuning & & & & \\
        Baseline+Tuning+append terminology & & & & \\
        Baseline+Tuning+XML markup & & & & \\
        Compound words segment& & & &\\
        Word order normalize & & & &\\
        Compound words segment+ word order normalize & & & &\\
    \end{tabular}
    \caption{Result}
    \label{tab:result}
\end{table}

\end{document}
