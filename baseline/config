################################################
### CONFIGURATION FILE FOR AN SMT EXPERIMENT ###
################################################


[GENERAL]
home-dir = /home/wqiu
exp-dir = /home/wqiu/work/wqiu/bmmt/
exp-name = baseline

working-dir = $home-dir/work/wqiu/bmmt/baseline
moses-src-dir = $home-dir/work/root/opt/moses
moses-script-dir = $moses-src-dir/scripts
moses-bin-dir = $moses-src-dir/bin
external-bin-dir = $home-dir/work/root/usr/bin 
data-dir = $exp-dir/data
train-dir = $data-dir/train
dev-dir = $data-dir/dev
test-dir = $data-dir/test
#test-dir = /home/wqiu/work/wqiu/thesis/nlg/mt/mosestalk/test-corpus-sp01/standard-padded
#test-dir-anno = /home/wqiu/work/wqiu/thesis/nlg/mt/mosestalk/test-corpus-sp01/anno-overlap/
#test-dir-276 = /home/wqiu/work/wqiu/thesis/nlg/mt/mosestalk/test-corpus/standard-padded-more-than-1-ref/
#test-dir = /home/wqiu/work/wqiu/thesis/nlg/mt/mosestalk/test-corpus-sp01/word-lattice-gt/
#test-dir-toy = /home/wqiu/work/wqiu/thesis/nlg/mt/mosestalk/test-corpus-sp01/word-lattice-gt-toy/
irstlm-dir = $home-dir/work/root/opt/irstlm/bin


ttable-binarizer = $moses-bin-dir/processPhraseTable
#binarize-all = $moses-script-dir/training/binarize-model.perl
decoder = $moses-bin-dir/moses

# we don't need the tokenizer for input
#input-tokenizer = "$moses-script-dir/tokenizer/tokenizer.perl -l $input-extension"
output-tokenizer = "$moses-script-dir/tokenizer/tokenizer.perl -l $output-extension"
#input-truecaser = $moses-script-dir/recaser/truecase.perl
output-truecaser = $moses-script-dir/recaser/truecase.perl
detruecaser = $moses-script-dir/recaser/detruecase.perl


input-extension = de
output-extension = en
#pair-extension = de-en

#################################################################
# PARALLEL CORPUS PREPARATION: 
# create a tokenized, sentence-aligned corpus, ready for training

[CORPUS]

max-sentence-length = 80

[CORPUS:baseline] 
raw-stem = $train-dir/train

[LM]

### tool to be used for language model training
# for instance: ngram-count (SRILM), train-lm-on-disk.perl (Edinburgh) 
# 
lm-training = "$moses-script-dir/generic/trainlm-irst2.perl -cores 6 -irst-dir $irstlm-dir -temp-dir $working-dir/tmp"
settings = "-s msb -p 0"

order = 3
type = 8
lm-binarizer = $moses-bin-dir/build_binary

[LM:baseline]
raw-corpus = $train-dir/train.$output-extension


#################################################################
# TRANSLATION MODEL TRAINING

[TRAINING]

### training script to be used: either a legacy script or 
# current moses training script (default) 
# 
script = $moses-script-dir/training/train-model.perl

### symmetrization method to obtain word alignments from giza output
# (commonly used: grow-diag-final-and)
#
#alignment-symmetrization-method = berkeley
alignment-symmetrization-method = grow-diag-final-and

### lexicalized reordering: specify orientation type
# (default: only distance-based reordering model)
#
lexicalized-reordering = msd-bidirectional-fe

# added to support word lattice
#training-options = "-num-lattice-features 1"

### if word alignment (giza symmetrization) should be skipped,
# point to word alignment files
#
#word-alignment = 

### if phrase extraction should be skipped,
# point to stem for extract files
#
#extracted-phrases = 

### if phrase table training should be skipped,
# point to phrase translation table
#
#phrase-translation-table = 

### if reordering table training should be skipped,
# point to reordering table
#
#reordering-table = 

### if training should be skipped, 
# point to a configuration file that contains
# pointers to all relevant model files
#
#config = 

### TUNING: finding good weights for model components

[TUNING]

### instead of tuning with this setting, old weights may be recycled

### tuning script to be used
#
tuning-script = $moses-script-dir/training/mert-moses.pl
tuning-settings = "-mertdir $moses-bin-dir "

### specify the corpus used for tuning 
# it should contain 100s if not 1000s of sentences
#
raw-input = $dev-dir/dev.$input-extension
 
raw-reference = $dev-dir/dev.$output-extension

### size of n-best list used (typically 100)
#
nbest = 100

### ranges for weights for random initialization
# if not specified, the tuning script will use generic ranges
# it is not clear, if this matters
#
# lambda = 

### additional flags for the decoder
#
# I've added -inputtype 2 to support lattice
#decoder-settings = "-threads 16 -inputtype 2 -max-phrase-length 60"

decoder-settings = "-threads 16" 
### if tuning should be skipped, specify this here
# and also point to a configuration file that contains
# pointers to all relevant model files
#
#config = 


#######################################################
## TRUECASER: train model to truecase corpora and input

[TRUECASER]

### script to train truecaser models
#
trainer = $moses-script-dir/recaser/train-truecaser.perl

### training data
# raw input needs to be still tokenized,
# also also tokenized input may be specified
#
raw-stem = CORPUS:raw-stem

### trained model
#
#truecase-model = 


##################################
## EVALUATION: score system output

[EVALUATION]

### prepare system output for scoring 
# this may include detokenization and wrapping output in sgm 
# (needed for nist-bleu, ter, meteor)
#
detokenizer = "$moses-script-dir/tokenizer/detokenizer.perl -l $output-extension"

decoder-settings = "-threads 4"

### should output be scored case-sensitive (default: no)?
#
# case-sensitive = yes
#
multiref = yes

### BLEU
#

multi-bleu = "$moses-script-dir/generic/multi-bleu.perl -lc"
# ibm-bleu =

### TER: translation error rate (BBN metric) based on edit distance
#
# ter = $edinburgh-script-dir/tercom_v6a.pl

### METEOR: gives credit to stem / worknet synonym matches
#
# meteor = 

#[EVALUATION:test-s21]
#raw-input = $test-dir/test.$pair-extension.$input-extension
#raw-reference = $test-dir/reference*

#[EVALUATION:sent-level]
#raw-input = $test-dir/test.$pair-extension.$input-extension
#raw-reference = $test-dir/reference

#[EVALUATION:anno]
#raw-input = $test-dir-anno/test.$pair-extension.$input-extension
#raw-reference = $test-dir-anno/reference

[EVALUATION:baseline]
raw-input = $test-dir/test.$input-extension
raw-reference = $test-dir/reference

#[EVALUATION:wordlattice-toy]
#raw-input = $test-dir-toy/test.$pair-extension.$input-extension
#raw-reference = $test-dir-toy/reference
#[EVALUATION:276test]
#raw-input = $test-dir-276/test.$pair-extension.$input-extension
#raw-reference = $test-dir-276/reference
[REPORTING]

### what to do with result (default: store in file evaluation/report)
# 
# email = pkoehn@inf.ed.ac.uk
